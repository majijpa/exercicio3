 # Cargar las librerias

import os
from osgeo import gdal, gdal_array
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import keras
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.models import Model, Sequential
from keras.layers import Activation
from keras.layers import Dense, Dropout, Input, BatchNormalization
from keras.layers import Conv1D, MaxPooling1D, Flatten, UpSampling1D, AveragePooling1D
from keras.layers import Conv2D, MaxPooling2D, Flatten
from keras.layers import Conv2DTranspose, Reshape
from tensorflow.python.keras import backend as K
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from keras.optimizers import Adam, Adagrad
from keras.metrics import categorical_crossentropy
from keras.utils import to_categorical, plot_model
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import precision_score
import torch
import torch.nn as nn
from keras import backend as K
from keras import  layers
import seaborn as sn
import time


# Carregar os dados e par√°metros
# dados de entrada treinamento
training_inputs = np.loadtxt("D:/DOCUMENTOS_IMPORTANTES/CURITIBA/DOUTORADO/SETIMO SEMESTRE/manuscrito_SAE_CNN_red/z_SALINAS/b204/inputs_salinas_204B.txt",int)

# dados de entrada treinamento
training_targets = np.loadtxt("D:/DOCUMENTOS_IMPORTANTES/CURITIBA/DOUTORADO/SETIMO SEMESTRE/manuscrito_SAE_CNN_red/z_SALINAS/b204/targets_salinas_204B.txt",int)

X=training_inputs
Y=training_targets



training_inputs,test_inputs,training_targets,test_targets = train_test_split(X,Y,test_size=0.30, random_state=20)

training_targets = to_categorical(training_targets)
test_targets = to_categorical(test_targets)


training_targets = np.delete(training_targets, 0, axis=1)
test_targets = np.delete(test_targets, 0, axis=1)

# reshape and normalize input images
nrows, ncols = training_inputs.shape
training_inputs = training_inputs.reshape(nrows, ncols, 1)
training_inputs = training_inputs.astype('float32') / 65535

nrows1, ncols1 = test_inputs.shape
test_inputs = test_inputs.reshape(nrows1, ncols1, 1)
test_inputs = test_inputs.astype('float32') / 65535


input_shape = (204,1)
batch_size = 32 # numero de imagens que o computador vai processar em cada passo
kernel_size = 7
latent_dim = 7
filters = 64
dropout = 0.2



# Estrutura da rede

# first build the encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = Conv1D(256,3, activation='relu', padding='same')(inputs)
x1 = MaxPooling1D(1)(x)
x2 = Conv1D(128,3, activation='relu', padding='same')(x1)
x3 = MaxPooling1D(1)(x2)
x4 = Conv1D(64,3, activation='relu', padding='same')(x3)
x5 = MaxPooling1D(1)(x4)
shape = K.int_shape(x5)
x6= Flatten()(x5)
latent = Dense(latent_dim, name='latent_vector')(x6)
encoder = Model(inputs, latent, name='encoder')
encoder.summary()



# build the decoder model
latent_inputs = Input(shape=(latent_dim,), name='decoder_input')
d1 = Dense(shape[1] * shape[2])(latent_inputs)
d2 = Reshape((shape[1], shape[2]))(d1)
d3 = Conv1D(64,3, activation='relu', padding='same')(d2)
d4 = UpSampling1D(1)(d3)
d5 = Conv1D(128,3, activation='relu', padding='same')(d4)
d6 = UpSampling1D(1)(d5)
d7 = Conv1D(256,3, activation='relu', padding='same')(d6)
d8 = UpSampling1D(1)(d7)
d9 = Conv1D(1,3, activation='relu',padding='same')(d8)
decoder = Model(latent_inputs, d9, name='decoder')
decoder.summary()



# autoencoder = encoder + decoder
# instantiate autoencoder model
autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')
autoencoder.summary()



autoencoder.compile(
    optimizer='adam',
    loss='mse',
)

# treinamento da rede

start_time = time.time()
# train the network
autoencoder.fit(
    training_inputs,
    training_inputs,
    # validation_data=(test_inputs, test_inputs),
    epochs=300,
    batch_size=32,
    shuffle=True
)

type(encoder)


# model = Sequential()
# for layers in encoder.layers:
#     model.add(layers)


# Remove last layer
encoder.layers.pop()

# 'Freeze' previous layers, so to single-train the new one
for layer in encoder.layers:
   layer.trainable = False


encoder.summary()



model = Sequential()
for layers in encoder.layers:
    model.add(layers)


model.summary()


# Append new layer + classification layer
model.add(Dense(300, activation="relu"))
model.add(Dense(16, activation='softmax'))

model.summary()

model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy',],)

model.fit(training_inputs, training_targets, validation_split=0.10, batch_size=128, epochs=100, shuffle=True, verbose=0)

#  Remove last layer
model.pop()

# 'Freeze' previous layers, so to single-train the new one
for layer in model.layers:
    layer.trainable = False

# Append new layer + classification layer
model.add(Dense(300, activation="relu"))
model.add(Dropout(0.2))
model.add(Dense(16, activation="softmax"))

model.summary()

model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy',],)

history=model.fit(training_inputs, training_targets, validation_split=0.20, batch_size=128, epochs=300, shuffle=True, verbose=0)

print("--- %s seconds ---" % (time.time() - start_time))


